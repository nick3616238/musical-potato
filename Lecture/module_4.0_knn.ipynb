{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "753fa553",
   "metadata": {},
   "source": [
    "## Part 1: The k-NN Classifier for Categorical Outcomes\n",
    "\n",
    "### Example: Riding Mowers\n",
    "\n",
    "### üè¢ Business Context\n",
    "\n",
    "Recall the riding mower marketing problem:\n",
    "- **Goal**: Predict which households will buy a riding mower\n",
    "- **Features**: Income ($1000s), Lot Size (1000s sq ft)\n",
    "- **Business value**: Target marketing to likely buyers\n",
    "\n",
    "**Why k-NN here?**\n",
    "- Relationship might be non-linear\n",
    "- \"Similar households (in income and lot size) have similar buying behavior\"\n",
    "- Small dataset (24 observations) ‚Üí k-NN works well\n",
    "\n",
    "### Step 1: Prepare Data (Train/Holdout Split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a2f00ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "library(ggplot2)\n",
    "library(ggrepel)\n",
    "library(caret)\n",
    "library(mlba)\n",
    "\n",
    "mowers.df <- mlba::RidingMowers\n",
    "set.seed(35)\n",
    "\n",
    "idx <- sample(nrow(mowers.df), 0.6*nrow(mowers.df))\n",
    "train.df <- mowers.df[idx, ]\n",
    "holdout.df <- mowers.df[-idx, ]\n",
    "\n",
    "## new household to classify\n",
    "new.df <- data.frame(Income = 60, Lot_Size = 20)\n",
    "\n",
    "cat(\"Training set:\", nrow(train.df), \"observations\\n\")\n",
    "cat(\"Holdout set:\", nrow(holdout.df), \"observations\\n\")\n",
    "cat(\"New household: Income =\", new.df$Income, \", Lot Size =\", new.df$Lot_Size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7f11702",
   "metadata": {},
   "source": [
    "### üìã Understanding the Data Split\n",
    "\n",
    "- **Training set** (60%): Used to find neighbors\n",
    "- **Holdout set** (40%): For evaluation (not used in this example)\n",
    "- **New household**: Income = $60K, Lot = 20,000 sq ft\n",
    "\n",
    "**Question**: Will this household buy a riding mower?\n",
    "\n",
    "### Visualizing the Data and New Point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ee76f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "g <- ggplot(mapping=aes(x=Income, y=Lot_Size, shape=Ownership, color=Ownership, fill=Ownership)) +\n",
    "  geom_point(data=train.df, size=4) +\n",
    "  geom_text_repel(aes(label=rownames(train.df)), data=train.df, show.legend = FALSE) +\n",
    "  geom_point(data=cbind(new.df, Ownership='New'),  size=5) +\n",
    "  scale_shape_manual(values = c(18, 15, 21)) +\n",
    "  scale_color_manual(values = c('black', 'darkorange', 'steelblue')) +\n",
    "  scale_fill_manual(values = c('black', 'darkorange', 'lightblue'))\n",
    "\n",
    "g"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d103ae91",
   "metadata": {},
   "source": [
    "### üìã Reading the k-NN Visualization\n",
    "\n",
    "**What you see**:\n",
    "- **Orange points**: Non-owners\n",
    "- **Blue points**: Owners  \n",
    "- **Black diamond**: New household (unknown ownership)\n",
    "- **Numbers**: Row indices from training data\n",
    "\n",
    "**Visual inspection**:\n",
    "- New point (60, 20) is surrounded by mostly **blue (owner)** points\n",
    "- Nearest neighbors appear to be owners\n",
    "- **Intuition**: This household will likely buy a mower\n",
    "\n",
    "**Key insight**: k-NN formalizes this visual intuition with distance calculations.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed190154",
   "metadata": {},
   "source": [
    "## Part 2: Building the k-NN Model\n",
    "\n",
    "### üè¢ Business Context: Choosing k\n",
    "\n",
    "The parameter **k** (number of neighbors) is critical:\n",
    "\n",
    "| k Value | Behavior | Business Impact |\n",
    "|---------|----------|----------------|\n",
    "| **k = 1** | Very sensitive to noise | Over-responds to outliers |\n",
    "| **k = 3-7** | Balanced | Good starting point |\n",
    "| **k = large** | Over-smoothed | Misses local patterns |\n",
    "\n",
    "**Trade-off**:\n",
    "- **Small k** ‚Üí Flexible, but noisy (overfitting)\n",
    "- **Large k** ‚Üí Stable, but too simple (underfitting)\n",
    "\n",
    "### Training k-NN with k=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "977f7b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "library(caret)\n",
    "# train k-NN model with k=3\n",
    "model <- train(Ownership ~ ., data=train.df,\n",
    "               method=\"knn\",  # specify the model\n",
    "               preProcess=c(\"center\", \"scale\"),  # normalize data\n",
    "               tuneGrid=expand.grid(k=3),\n",
    "               trControl=trainControl(method=\"none\"))\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b926e740",
   "metadata": {},
   "source": [
    "### üìã Understanding the Model Output\n",
    "\n",
    "**Key components**:\n",
    "- **k-Nearest Neighbors**: Model type\n",
    "- **15 samples, 2 predictors, 2 classes**: Data summary\n",
    "- **Pre-processing: centered (2), scaled (2)**: Features were normalized\n",
    "- **k = 3**: Using 3 nearest neighbors\n",
    "\n",
    "### ‚ö†Ô∏è CRITICAL: Why Normalize?\n",
    "\n",
    "**Problem without normalization**:\n",
    "\n",
    "```\n",
    "Household A: Income = 60 ($60K),  Lot = 20 (20K sq ft)\n",
    "Household B: Income = 65 ($65K),  Lot = 18 (18K sq ft)\n",
    "\n",
    "Distance = ‚àö[(65-60)¬≤ + (18-20)¬≤]\n",
    "         = ‚àö[25 + 4] = ‚àö29 ‚âà 5.4\n",
    "\n",
    "Income difference: 5\n",
    "Lot difference: 2\n",
    "\n",
    "PROBLEM: Income dominates distance just because of scale!\n",
    "But 2K sq ft difference might be MORE important than $5K income.\n",
    "```\n",
    "\n",
    "**Solution: Normalize** (mean=0, sd=1 for both variables)\n",
    "\n",
    "```\n",
    "After normalization:\n",
    "Income: (60 - mean) / sd\n",
    "Lot:    (20 - mean) / sd\n",
    "\n",
    "Now both contribute equally to distance calculation.\n",
    "```\n",
    "\n",
    "**Best practice**: **Always** use `preProcess=c(\"center\", \"scale\")` for k-NN!\n",
    "\n",
    "### Making Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ed01bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict new data point\n",
    "predict(model, new.df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cea98ae",
   "metadata": {},
   "source": [
    "### üìã Interpreting the Prediction\n",
    "\n",
    "**Output**: \"Owner\" (or \"Nonowner\")\n",
    "\n",
    "**What happened behind the scenes**:\n",
    "1. Normalized new household's features\n",
    "2. Calculated distance to all 15 training observations\n",
    "3. Found 3 nearest neighbors\n",
    "4. Checked their classes ‚Üí majority vote\n",
    "5. Returned winning class\n",
    "\n",
    "**Business action**: If \"Owner\" ‚Üí Add to marketing campaign!\n",
    "\n",
    "### Identifying the Nearest Neighbors\n",
    "\n",
    "Let's see **which specific households** influenced the prediction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88df41ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# determine nearest neighbors to new data point\n",
    "train.norm.df <- predict(model$preProcess, train.df)\n",
    "new.norm.df <- predict(model$preProcess, new.df)\n",
    "distances <- apply(train.norm.df[, 1:2], 1,\n",
    "                   function(d){ sqrt(sum((d - new.norm.df)^2)) })\n",
    "rownames(train.df)[order(distances)][1:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db8fc262",
   "metadata": {},
   "source": [
    "### üìã Understanding Nearest Neighbors\n",
    "\n",
    "**Output**: Row indices of 3 closest households\n",
    "\n",
    "**What this tells you**:\n",
    "- These are the 3 most similar households from training data\n",
    "- Their ownership status determined the prediction\n",
    "- You can examine these specific cases to understand the prediction\n",
    "\n",
    "**Business value**:\n",
    "- **Explainability**: \"We predict Owner because households #4, #8, #12 are very similar and all own mowers\"\n",
    "- **Stakeholder confidence**: Show actual comparable cases\n",
    "- **Quality check**: If neighbors seem unreasonable, investigate data issues\n",
    "\n",
    "### üè¢ Comparing to Real Estate Comps\n",
    "\n",
    "This is exactly like real estate \"comparables\":\n",
    "\n",
    "```\n",
    "PROPERTY VALUATION\n",
    "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "Subject Property: 60K income, 20K sq ft lot\n",
    "\n",
    "Comparable 1: 58K income, 19K sq ft ‚Üí Owns mower\n",
    "Comparable 2: 62K income, 21K sq ft ‚Üí Owns mower  \n",
    "Comparable 3: 59K income, 20K sq ft ‚Üí Owns mower\n",
    "\n",
    "Conclusion: Subject likely to own mower\n",
    "```\n",
    "\n",
    "Stakeholders understand this reasoning!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cf2bcf0",
   "metadata": {},
   "source": [
    "## Part 3: Choosing the Optimal k\n",
    "\n",
    "### üè¢ Business Context: Hyperparameter Tuning\n",
    "\n",
    "How do we know k=3 is the best choice? We need to **test different values** and see which performs best.\n",
    "\n",
    "**Challenge**: Our dataset is small (24 observations)\n",
    "- 60% training = 15 observations\n",
    "- Can't afford to hold out more data for validation\n",
    "\n",
    "**Solution**: **Leave-One-Out Cross-Validation (LOOCV)**\n",
    "\n",
    "### How LOOCV Works\n",
    "\n",
    "```\n",
    "For each observation in training set:\n",
    "  1. Remove it temporarily (\"leave one out\")\n",
    "  2. Train model on remaining 14 observations\n",
    "  3. Predict the removed observation\n",
    "  4. Check if prediction is correct\n",
    "  \n",
    "Repeat for all 15 observations ‚Üí 15 predictions\n",
    "Accuracy = % correct predictions\n",
    "```\n",
    "\n",
    "**Advantage**: Uses ALL data for both training and validation (no waste!)\n",
    "\n",
    "### Testing Multiple k Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dbf2857",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use leave-one-out cross-validation for small dataset\n",
    "trControl <- trainControl(method=\"loocv\", number=5, allowParallel=TRUE)\n",
    "model <- train(Ownership ~ ., data=train.df,\n",
    "               method=\"knn\",\n",
    "               preProcess=c(\"center\", \"scale\"),\n",
    "               tuneGrid=expand.grid(k=seq(1, 13, 2)),\n",
    "               trControl=trControl)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e79469",
   "metadata": {},
   "source": [
    "### üìã Interpreting the Tuning Results\n",
    "\n",
    "**What you see**:\n",
    "- **tuneGrid**: Tested k = 1, 3, 5, 7, 9, 11, 13\n",
    "- **Accuracy** for each k value\n",
    "- **Best k** is highlighted (highest accuracy)\n",
    "\n",
    "**How to read the results**:\n",
    "\n",
    "| k | Accuracy | Interpretation |\n",
    "|---|----------|----------------|\n",
    "| 1 | Lower | Too sensitive to outliers |\n",
    "| 3-7 | **Highest** | Sweet spot ‚úì |\n",
    "| 11-13 | Lower | Over-smoothing |\n",
    "\n",
    "### üìä Typical k Selection Pattern\n",
    "\n",
    "```\n",
    "ACCURACY\n",
    "  ^\n",
    "  ‚îÇ     ‚ï±‚Äæ‚ï≤\n",
    "  ‚îÇ    ‚ï±   ‚ï≤___\n",
    "  ‚îÇ   ‚ï±        ‚ï≤___\n",
    "  ‚îÇ  ‚ï±             ‚ï≤___\n",
    "  ‚îÇ ‚ï±                  ‚ï≤\n",
    "  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ> k\n",
    "    1  3  5  7  9  11  13\n",
    "    \n",
    "    Low k:  Overfitting (noisy)\n",
    "    Mid k:  Just right ‚úì\n",
    "    High k: Underfitting (too simple)\n",
    "```\n",
    "\n",
    "**Business decision**: Use the k with highest cross-validation accuracy.\n",
    "\n",
    "### Training Final Model with Optimal k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88300dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the best k found from cross-validation (assume k=7 was best)\n",
    "model <- train(Ownership ~ ., data=mowers.df,\n",
    "               method=\"knn\",\n",
    "               preProcess=c(\"center\", \"scale\"),\n",
    "               tuneGrid=expand.grid(k=7),\n",
    "               trControl=trainControl(method=\"none\"))\n",
    "predict(model, new.df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f38e9d8",
   "metadata": {},
   "source": [
    "### üìã Why Retrain on Full Data?\n",
    "\n",
    "**After finding optimal k**:\n",
    "1. We used training set (15 obs) to find k=7 is best\n",
    "2. Now retrain on **ALL data** (24 obs) with k=7\n",
    "3. More data ‚Üí Better final model\n",
    "\n",
    "**Workflow**:\n",
    "```\n",
    "Step 1: Use subset + LOOCV ‚Üí Find best k\n",
    "Step 2: Use full data + best k ‚Üí Final model\n",
    "Step 3: Predict new cases\n",
    "```\n",
    "\n",
    "**Business rationale**: We already paid the cost of tuning (finding k). Now maximize model quality by using all available data.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f1740a5",
   "metadata": {},
   "source": [
    "## Part 4: Examining Neighbors for Business Insights\n",
    "\n",
    "### üè¢ Business Context: Building Trust\n",
    "\n",
    "Stakeholders often ask: **\"Why did you classify this household as an Owner?\"**\n",
    "\n",
    "With k-NN, you can show the **actual comparable cases**. This builds trust and helps validate the model.\n",
    "\n",
    "### Finding All Neighbors Within k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41dea09e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's examine the closest 8 neighbors (more than k=7 for context)\n",
    "train.norm.df <- predict(model$preProcess, train.df)\n",
    "new.norm.df <- predict(model$preProcess, new.df)\n",
    "distances <- apply(train.norm.df[, 1:2], 1,\n",
    "                   function(d){ sqrt(sum((d - new.norm.df)^2)) })\n",
    "train.df[order(distances)[1:8],]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d9b3f9f",
   "metadata": {},
   "source": [
    "### üìã Interpreting the Neighbors Table\n",
    "\n",
    "**What you see**: 8 closest households sorted by distance (closest first)\n",
    "\n",
    "**Columns**:\n",
    "- **Income**: Annual household income ($1000s)\n",
    "- **Lot_Size**: Property size (1000s sq ft)\n",
    "- **Ownership**: Actual ownership status\n",
    "\n",
    "**How to use this**:\n",
    "\n",
    "1. **Check consistency**: Are most neighbors the same class?\n",
    "   - If 7/8 are \"Owner\" ‚Üí High confidence\n",
    "   - If 4/8 are \"Owner\" ‚Üí Low confidence (borderline case)\n",
    "\n",
    "2. **Examine outliers**: Is there an unexpected neighbor?\n",
    "   - Very different Income/Lot but still \"close\"?\n",
    "   - Might indicate data quality issue or interesting pattern\n",
    "\n",
    "3. **Business reasoning**: Show to stakeholders\n",
    "   - \"These 7 similar households all own mowers\"\n",
    "   - \"Comparable cases support our classification\"\n",
    "\n",
    "### üè¢ Real-World Application: Loan Approval Explanation\n",
    "\n",
    "```\n",
    "LOAN APPLICATION DECISION\n",
    "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "Applicant: $60K income, $20K savings, 680 credit score\n",
    "Decision: APPROVED\n",
    "\n",
    "Rationale:\n",
    "We examined 7 similar applicants:\n",
    "  - Similar income (58-62K)\n",
    "  - Similar savings (18-22K)  \n",
    "  - Similar credit (670-690)\n",
    "  \n",
    "6 out of 7 successfully repaid loans ‚Üí Low risk\n",
    "1 defaulted (outlier, had additional risk factors)\n",
    "\n",
    "Recommendation: Approve with standard terms\n",
    "```\n",
    "\n",
    "This is **much more convincing** than \"the algorithm said yes.\"\n",
    "\n",
    "### Investigating Borderline Cases\n",
    "\n",
    "**When neighbors disagree** (e.g., 4 Owner, 3 Nonowner):\n",
    "\n",
    "| Action | Purpose |\n",
    "|--------|----------|\n",
    "| **Collect more data** | New household might genuinely be borderline |\n",
    "| **Flag for review** | Human expert should examine |\n",
    "| **Use probabilities** | Report confidence (e.g., 57% Owner) |\n",
    "| **Gather more features** | Need additional info to differentiate |\n",
    "\n",
    "**Business value**: Uncertainty quantification helps avoid costly mistakes.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a15cbe7",
   "metadata": {},
   "source": [
    "## Summary: Key Takeaways\n",
    "\n",
    "### üîß Essential k-NN Functions\n",
    "\n",
    "#### Building k-NN Models\n",
    "| Task | Function | Critical Parameters |\n",
    "|------|----------|--------------------|\n",
    "| Train k-NN | `train(y ~ ., data, method='knn')` | Always include `preProcess` |\n",
    "| Normalize | `preProcess=c('center', 'scale')` | **REQUIRED** for k-NN |\n",
    "| Set k | `tuneGrid=expand.grid(k=3)` | Choose via cross-validation |\n",
    "| Tune k | `trControl=trainControl(method='loocv')` | For small datasets |\n",
    "\n",
    "#### Making Predictions\n",
    "| Task | Function | Output |\n",
    "|------|----------|--------|\n",
    "| Predict class | `predict(model, newdata)` | Class labels |\n",
    "| Find neighbors | Compute distances manually | Explainability |\n",
    "\n",
    "---\n",
    "\n",
    "### üéØ k-NN Best Practices Checklist\n",
    "\n",
    "‚úÖ **ALWAYS normalize features** with `preProcess=c('center', 'scale')`\n",
    "   - k-NN is distance-based ‚Üí scale matters!\n",
    "   - Without normalization, results will be wrong\n",
    "\n",
    "‚úÖ **Choose k via cross-validation**\n",
    "   - Don't guess k arbitrarily\n",
    "   - Use LOOCV for small datasets\n",
    "   - Use k-fold CV for larger datasets\n",
    "\n",
    "‚úÖ **Start with odd k** to avoid ties\n",
    "   - k=3, 5, 7 are good starting points\n",
    "   - Avoids 50/50 splits in binary classification\n",
    "\n",
    "‚úÖ **Examine nearest neighbors** for explainability\n",
    "   - Show stakeholders comparable cases\n",
    "   - Validate predictions make sense\n",
    "   - Identify borderline/uncertain cases\n",
    "\n",
    "‚úÖ **Watch for curse of dimensionality**\n",
    "   - k-NN degrades with many features\n",
    "   - Consider PCA for dimension reduction first\n",
    "   - Or use feature selection\n",
    "\n",
    "‚úÖ **Consider computational cost**\n",
    "   - Prediction requires distance to ALL training points\n",
    "   - Slow for large datasets (100K+ rows)\n",
    "   - May need approximate methods or different algorithm\n",
    "\n",
    "---\n",
    "\n",
    "### üìä Choosing k: Guidelines\n",
    "\n",
    "| k Range | Behavior | When to Use |\n",
    "|---------|----------|-------------|\n",
    "| **k = 1** | Memorizes training data | Almost never (overfits) |\n",
    "| **k = 3-7** | Balanced, flexible | **Start here** ‚úì |\n",
    "| **k = ‚àön** | Rule of thumb | Medium datasets |\n",
    "| **k = n/10** | Conservative | Noisy data |\n",
    "| **k = large** | Over-smoothed | Very clean data only |\n",
    "\n",
    "**Best practice**: Let cross-validation decide, but sanity-check the result.\n",
    "\n",
    "---\n",
    "\n",
    "### üè¢ Business Value Summary\n",
    "\n",
    "| k-NN Feature | Business Benefit | Example |\n",
    "|--------------|------------------|----------|\n",
    "| **Similarity-based** | Intuitive explanations | \"Similar customers bought this\" |\n",
    "| **Non-linear** | Captures complex patterns | Flexible decision boundaries |\n",
    "| **Lazy learning** | No training time | Instant model updates |\n",
    "| **Explainable neighbors** | Stakeholder trust | Show comparable cases |\n",
    "| **Probability estimates** | Risk quantification | Confidence scores |\n",
    "\n",
    "### üö® Common k-NN Pitfalls\n",
    "\n",
    "| Mistake | Consequence | Fix |\n",
    "|---------|-------------|-----|\n",
    "| **Forget to normalize** | Wrong predictions | Always `preProcess=c('center','scale')` |\n",
    "| **Arbitrary k** | Suboptimal performance | Use cross-validation |\n",
    "| **Too many features** | Distances meaningless | PCA or feature selection |\n",
    "| **Categorical features** | Distance undefined | Use dummy encoding |\n",
    "| **Large dataset** | Slow predictions | Consider alternatives |\n",
    "| **Outliers in data** | Skew normalization | Remove or transform |\n",
    "\n",
    "---\n",
    "\n",
    "### üìä k-NN vs. Other Classifiers\n",
    "\n",
    "**When k-NN wins**:\n",
    "- ‚úÖ Non-linear relationships\n",
    "- ‚úÖ Need to show comparable cases\n",
    "- ‚úÖ Small to medium data\n",
    "- ‚úÖ Continuous features\n",
    "\n",
    "**When to use alternatives**:\n",
    "- Large data (100K+ rows) ‚Üí **Decision Trees, Random Forest**\n",
    "- Need interpretable rules ‚Üí **Decision Trees, Logistic Regression**\n",
    "- High dimensions (100+ features) ‚Üí **Regularized Logistic, SVM**\n",
    "- Real-time predictions ‚Üí **Pre-trained models (LDA, Logistic)**\n",
    "- Linear relationships ‚Üí **Logistic Regression, LDA**\n",
    "\n",
    "---\n",
    "\n",
    "### üí° Advanced k-NN Techniques\n",
    "\n",
    "| Technique | Purpose | When to Use |\n",
    "|-----------|---------|-------------|\n",
    "| **Weighted k-NN** | Closer neighbors count more | When distance matters |\n",
    "| **Distance metrics** | Manhattan, Euclidean, etc. | Different data types |\n",
    "| **Feature weighting** | Some features more important | Domain knowledge available |\n",
    "| **Approximate NN** | Speed up large datasets | Production systems |\n",
    "\n",
    "---\n",
    "\n",
    "### üìö Connection to Other Modules\n",
    "\n",
    "- **Module 1.1**: Train/test splits apply to k-NN\n",
    "- **Module 2.1 (LDA)**: Compare linear vs. non-linear boundaries\n",
    "- **Module 2.2 (PCA)**: Reduce dimensions before k-NN\n",
    "- **Module 3**: Distance metrics used in both clustering and k-NN\n",
    "- **Module 5**: Clustering is unsupervised k-NN\n",
    "\n",
    "---\n",
    "\n",
    "### üéì k-NN Interview Questions\n",
    "\n",
    "**For stakeholders**:\n",
    "- \"How does k-NN work?\" ‚Üí \"It finds similar historical cases and uses their outcomes\"\n",
    "- \"Why this prediction?\" ‚Üí Show the k nearest neighbors as evidence\n",
    "- \"How confident are you?\" ‚Üí \"X out of k neighbors agree\"\n",
    "\n",
    "**For technical teams**:\n",
    "- \"Did you normalize?\" ‚Üí Yes, with `preProcess=c('center','scale')`\n",
    "- \"How did you choose k?\" ‚Üí Cross-validation tested k=1,3,5,7,9; k=5 was best\n",
    "- \"Computational cost?\" ‚Üí O(n) per prediction, manageable for our data size\n",
    "\n",
    "---\n",
    "\n",
    "**Next Steps**:\n",
    "1. Apply k-NN to your own classification problems\n",
    "2. Always normalize features first!\n",
    "3. Use cross-validation to find optimal k\n",
    "4. Show nearest neighbors to stakeholders for explainability\n",
    "5. Compare k-NN performance to logistic regression and decision trees"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
